{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://hub.packtpub.com/generating-automated-image-captions-using-nlp-and-computer-vision-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'enable_eager_execution'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1106b14e7ffa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import TensorFlow and enable eager execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'enable_eager_execution'"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow and enable eager execution\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image and Caption Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = os.path.abspath('cocoapi/annotations/captions_train2014.json')\n",
    "PATH = os.path.abspath('/Users/bwilke/Desktop/images/train2014/') # way to big for my cloud storage, so point to local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json annotation file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the captions and the image name in vectors\n",
    "all_captions = []\n",
    "all_img_name_vector = []\n",
    "\n",
    "# extract caption and image ID to form full path from raw JSON annotations\n",
    "for annot in annotations['annotations']:\n",
    "    caption = ' ' + annot['caption'] + ' '\n",
    "    image_id = annot['image_id']\n",
    "    full_coco_image_path = PATH + 'COCO_train2014_{}.jpg'.format(image_id)\n",
    "\n",
    "# adds the path and corresponding caption to shared index across 2 lists\n",
    "all_img_name_vector.append(full_coco_image_path)\n",
    "all_captions.append(caption)\n",
    "\n",
    "# shuffling the captions and image_names together; setting a random state\n",
    "train_captions, img_name_vector = shuffle(all_captions, all_img_name_vector, random_state = 1)\n",
    "\n",
    "# selecting the first 40000 captions/images from the shuffled sey by replacing with sliced set\n",
    "num_examples = 40000\n",
    "train_captions = train_captions[:num_examples]\n",
    "img_name_vector = img_name_vector[:num_examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Loader, CNN Model, and Image Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.read_file(image_path)                                   # gets image from path    \n",
    "    img = tf.image.decode_jpeg(img, channels=3)                      # decodes JPG\n",
    "    img = tf.image.resize_images(img, (299, 299))                    # resize\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)   # preprocess for inceptionV3\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Inception-V3 with ImageNet weights\n",
    "# this is our CNN where output layer is the last layer in Inception-V3\n",
    "\n",
    "image_model = tf.keras.applications.InceptionV3(include_top = False, weights='imagenet')  # loads pretrained model\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "# image_features_extract_model is our deep CNN encoder, which is responsible for learning\n",
    "# the features from the given image.\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_train = sorted(set(img_name_vector))                # sort unique entries in img_name_vector\n",
    "\n",
    "# for each image path in encode_train, apply load_image(), returns tensor\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train).map(load_image).batch(16)\n",
    "\n",
    "# Extract features\n",
    "for img, path in image_dataset:\n",
    "    batch_features = image_features_extract_model(img)           # forward feed the image tensor through the model\n",
    "    # reshape the output\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "# save each output to disk \n",
    "for bf, p in zip(batch_features, path):\n",
    "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "    np.save(path_of_feature, bf.numpy())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for RNN Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper func to find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "# Performing tokenization on the top 5000 words from the vocabulary\n",
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, \n",
    "oov_token=\"\", \n",
    "filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "\n",
    "# Converting text into sequence of numbers\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "tokenizer.word_index = {key:value for key, value in tokenizer.word_index.items() if value # putting  token in the word2idx dictionary\n",
    "tokenizer.word_index[tokenizer.oov_token] = top_k + 1\n",
    "tokenizer.word_index[''] = 0\n",
    "\n",
    "# creating the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "# creating a reverse mapping (index -> word)\n",
    "index_word = {value:key for key, value in tokenizer.word_index.items()}\n",
    "\n",
    "# padding each vector to the max_length of the captions\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "# calculating the max_length used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result is a list of lists containing our encoded captions\n",
    "cap_vector[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 80/20 training and validation, where images are X and captions are Y\n",
    "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector, cap_vector, test_size = 0.2, random_state = 69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the sample counts\n",
    "print (\"No of Training Images:\",len(img_name_train))\n",
    "print (\"No of Training Caption: \",len(cap_train))\n",
    "print (\"No of Training Images\",len(img_name_val))\n",
    "print (\"No of Training Caption:\",len(cap_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "# Defining parameters\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the vector extracted from Inception-V3 is (64, 2048)\n",
    "# these two variables represent that\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "\n",
    "# loading the numpy files \n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap\n",
    "\n",
    "#We use the from_tensor_slices to load the raw data and transform them into the tensors\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Using the map() to load the numpy files in parallel\n",
    "# NOTE: Make sure to set num_parallel_calls to the number of CPU cores you have\n",
    "# https://www.tensorflow.org/api_docs/python/tf/py_func\n",
    "dataset = dataset.map(lambda item1, item2: tf.py_func(map_func, [item1, item2], [tf.float32, tf.int32]), num_parallel_calls=8)\n",
    "\n",
    "# shuffling and batching\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Captioning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features extracted from lower convolutional layer of Inception-v3 is (8,8,2048)\n",
    "\n",
    "This is unrolled to (64,2048)\n",
    "\n",
    "Then passed to single dense layer of CNN encoder, RNN attends over image to predict next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    if tf.test.is_gpu_available():\n",
    "        return tf.keras.layers.CuDNNGRU(units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, return_sequences=True, return_state=True, recurrent_activation='sigmoid', recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the attention mechanism known as Bahdanau attention.\n",
    "\n",
    "Takes the features from the CNN encoder of a shape of (batch_size, 64, embedding_dim).\n",
    "\n",
    "This attention mechanism will return the context vector and the attention weights over the time axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, features, hidden): # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Captioning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, extract features we saved to .npy files and pass each through CNN encoder.\n",
    "\n",
    "Then encoder output, hidden state(init to 0), and the decoder input (which is start token) are passed to decoder.\n",
    "\n",
    "Decoder returns predictions and the decoder hidden state.\n",
    "\n",
    "Decoder hidden state is passed back to the model and predictions are used to calculate loss.\n",
    "\n",
    "Teacher forcing technique is used to decide next input to decoder.\n",
    "\n",
    "Finally, calculate gradient and apply it the optimizer and back prop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "\n",
    "        # initializing the hidden state for each batch because the captions are not related from image to image\n",
    "        hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "        dec_input = tf.expand_dims([tokenizer.word_index['']] * BATCH_SIZE, 1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            features = encoder(img_tensor)\n",
    "\n",
    "            for i in range(1, target.shape[1]): # passing the features through the decoder\n",
    "                predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "                loss += loss_function(target[:, i], predictions)\n",
    "                dec_input = tf.expand_dims(target[:, i], 1)         # using teacher forcing\n",
    "                total_loss += (loss / int(target.shape[1]))\n",
    "\n",
    "        variables = encoder.variables + decoder.variables\n",
    "\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, variables), tf.train.get_or_create_global_step())\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, loss.numpy() / int(target.shape[1])))\n",
    "\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / len(cap_vector))\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss/len(cap_vector)))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
